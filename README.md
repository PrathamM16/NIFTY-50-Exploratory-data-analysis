**Preprocessed Dataset README**
This repository contains a preprocessed dataset with various operations applied for analysis and machine learning tasks.

**Table of Contents**
Introduction
Data Loading and Exploration
Understanding Data
Handling Missing Data
Exploratory Data Analysis
Encoding Categorical Data
Feature Scaling
Normalization
Splitting Dataset
Saving Preprocessed Dataset
Contributing

**Introduction**
This repository contains a dataset that has undergone various preprocessing operations essential for data analysis and machine learning tasks. The operations include data loading, exploration, handling missing data, exploratory data analysis, encoding categorical data, feature scaling, normalization, and dataset splitting.

**Data Loading and Exploration**
Loading the Dataset: The dataset was loaded using Pandas from a CSV file.
Exploratory Analysis: Various graphs were plotted to visualize relationships and distributions within the dataset.
Example Plots:
Line plot showing trends over time.
Histograms depicting distribution of numerical variables.
Scatter plots for pairwise relationships.
Bar plots for categorical variables.

**Understanding Data**
Number of Samples and Attributes: Displayed the number of rows (samples) and columns (attributes) in the dataset.
Column Names: Listed all the column names in the dataset.
Data Frame Structure: Described the structure of the data frame.
Statistical Information: Provided statistical summaries of the dataset.
Count of Unique Values: Identified and counted unique values in specific columns.

**Handling Missing Data**
Identified Missing Values: Determined the presence of missing values in the dataset.
Handling Strategies: Removed rows with missing values and replaced missing values with appropriate alternatives.

**Exploratory Data Analysis**
Categorical Data Encoding: Applied LabelEncoder to encode categorical data for numerical analysis.
Feature Scaling: Performed feature scaling using Standardization (Z-score) technique.
Normalization: Conducted normalization using Min-Max scaling and explained differences from standardization.
Dataset Splitting: Split the dataset into training and test sets with a 75%-25% ratio, highlighting the importance of random_state parameter.

**Encoding Categorical Data**
Label Encoding: Applied LabelEncoder from scikit-learn to transform categorical data into numerical values.
**Feature Scaling**
Standardization: Explained the process of standardization (Z-score normalization) and its computational method.

**Normalization**
Min-Max Scaling: Described normalization using Min-Max scalar and explained its differences from standardization.
**Splitting Dataset**
Train-Test Split: Split the dataset into training (75%) and testing (25%) sets, emphasizing the role of random_state parameter.

**Saving Preprocessed Dataset**
The preprocessed dataset was saved in CSV format for further analysis and modeling tasks.

**Contributing**
Contributions to enhance the preprocessing methods or improve documentation are welcome. Follow these steps to contribute:

Create a new branch (git checkout -b feature-branch)
Make changes and commit (git commit -am 'Add new feature')
Push to the branch (git push origin feature-branch)
Submit a pull request
![image](https://github.com/PrathamM16/NIFTY-50-Exploratory-data-analysis/assets/121935421/e7415f24-16e4-426a-bb12-4bdd475d7c73)
